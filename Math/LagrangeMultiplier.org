* Using tangency to solve constrained optimization
:Reference:
https://www.youtube.com/watch?v=yuqB-d5MjZA&t=1s
:END:



#+DOWNLOADED: /tmp/screenshot.png @ 2017-05-20 09:58:32
[[file:Using tangency to solve constrained optimization/screenshot_2017-05-20_09-58-32.png]]


#+DOWNLOADED: /tmp/screenshot.png @ 2017-05-20 09:54:19
[[file:Using tangency to solve constrained optimization/screenshot_2017-05-20_09-54-19.png]]

#+DOWNLOADED: /tmp/screenshot.png @ 2017-05-20 09:54:32
[[file:Using tangency to solve constrained optimization/screenshot_2017-05-20_09-54-32.png]]

obj: maximize f(x,y)=x^2y
sbj: on the set x^2 + y^2 = 1

先把 obj 和 sbj 想象成两个函数：
obj: f(x,y) = x^2y
sbj: g(x,y) = x^2 + y^2
上两张图展示的就是两个函数固定其中任何一个，让另一个
取不同的f值 or g值 所产生的不同contour.

那么显然两者的gradient在相切的那点应该是相等的
gradf(xm,ym) = gradg(xm,ym)


#+DOWNLOADED: /tmp/screenshot.png @ 2017-05-20 10:05:27
[[file:Using tangency to solve constrained optimization/screenshot_2017-05-20_10-05-27.png]]


#+DOWNLOADED: /tmp/screenshot.png @ 2017-05-20 10:05:50
[[file:Using tangency to solve constrained optimization/screenshot_2017-05-20_10-05-50.png]]


#+DOWNLOADED: /tmp/screenshot.png @ 2017-05-20 10:06:29
[[file:Using tangency to solve constrained optimization/screenshot_2017-05-20_10-06-29.png]]
