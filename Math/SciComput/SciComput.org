* SciComput
** Basic Computations and Visualization
*** 5 Basic Optimization
    Optimization methods are prevalent throughout a vast range of applications.
    In its simplest form, an optimization routine simply attempts to find the
    maximum or minimum of a real-valued function, i.e. the objective function,
    by developing an algorithm that systematically chooses input values from an
    allowed set, or feasible set, and computes the value of the function.

    Typically, the _optimization algorithm_ is built upon an _iteration scheme_
    that continues to choose new input values so that the maximum or minimum of
    the objective function is achieved.

*** 5.1 Unconstrained Optimization (Derivative-Free Methods)
    The concept of optimization is fairly simple: find the minimum or maximum
    of a function. In optimization, the function is real-valued and called the
    _objective function._

    Moreover, it is often the case that you would like to find the maximum or
    minimum under some constraints on the input values to the function.

    Thus there is a concept of _feasibility_ of the solution. To begin, we will
    consider optimization without constraints, thus the aptly named
    unconstrained optimization problem.

    In this section, however, derivative-free methods for computing the minimum
    or maximum will be considered.

    Such methods are often BASIC OPTIMIZATION employed when
       - computing the derivatives is either very expensive
       - simply intractable in practice.

    Note that in this definition of the minimum, it is not specified wether
    this is a local or global minimum. For highly nontrivial functions f(x), it
    may be that there _exists many local minima_, thus a search algorithm that
    converges to a minimum solution may not be the actual desired solution.

    The methods here are very much like the root solving techniques
    demonstrated in the earlier chapters of the book, i.e. they are simple
    _iterative schemes_ that progressively produce better and better minimization
    values.

    In both methods shown, an _initial interval_ is required in which the minimum
    lies.
**** Golden section search
     _Given an interval in which the minimum is known to exist_, the golden
     section search algorithm is an efficient method for finding the minimum of
     the function.
***** 算法思想
      The golden section search gives a simple procedure for selecting the
      evaluation points x1 and x2. Specifically, they are chosen so that

      1) we want a constant _reduction factor_, say c, for the size of the
         interval;
      2) they can be _reused_ at the next iteration, thus saving a
         function evaluation.

      To be more specific, _the function must be unimodal(单峰的) in the interval_

      x ∈ [a, b]

      meaning there is only a single relative minimum in the interval chosen.
      The algorithm is much like _the bisection method for root solving._

      Specifically, within the interval where the minimum exists, two points
      (x1 and x2) are chosen so that

      a < x1 < x2 < b.

      Evaluation of the function f (x) is then performed at x1 and x2. The
      following observations hold:

      if f (x1) ≤ f (x2) then retain interval x ∈ [a, x2]
      if f (x1) > f (x2) then retain interval x ∈ [x1, b].

      This ensures that the minimum is now in a _smaller subinterval_ of the
      original x ∈ [a, b].

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-04 18:54:59
      [[file:SciComput/screenshot_2017-05-04_18-54-59.png]]

      Figure : Graphical depiction of the golden section search. (a) Two
      points, x1 and x2, are used for the iteration process. The points are
      chosen so as to keep the ratio of (b − x1)/(b − a) and (x2 − a)/(b − a)
      the same. (b) The first three iterations are depicted.

***** 数学推论过程
      How to get C?
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-04 18:59:00
      [[file:SciComput/screenshot_2017-05-04_18-59-00.png]]
      where c is the reduction factor to the new intervals
      x ∈ [a, x2] or
      x ∈ [x1, b] respectively (see Fig. 5.1)

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-04 18:59:50
      [[file:SciComput/screenshot_2017-05-04_18-59-50.png]]
      The key observation is the following: if f (x1) < f (x2) then the new
      interval is x ∈ [a, x2]. Moreover, a new x1 must be evaluated while the
      old x2 is the x1.

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-04 19:00:16
      [[file:SciComput/screenshot_2017-05-04_19-00-16.png]]
      where only the positive root of c is kept since c must be positive. Note
      that the resulting ratio 'c' is simply unity minus the golden ratio, i.e.

      φ = 1 + c

      thus the name _golden section search_. Now that c is determined, x1 and x2
      can be found from.

***** Matlab 代码示例
      The following finds the minimum of the function
      [[file:SciComput/screenshot_2017-05-04_19-12-43.png]]
      x ∈ [−2, 1].
      #+NAME: goldensearch.m
      #+BEGIN_SRC matlab
     a=-2; b=1; % initial interval
     c=(-1+sqrt(5))/2; % golden section

     x1=c*a + (1-c)*b;
     x2=(1-c)*a + c*b;
     f1=x1.^4+10*x1.*sin(x1.^2);
     f2=x2.^4+10*x2.*sin(x2.^2);

     for j=1:100
         if f1<f2 % move right boundary
             b=x2; x2=x1; f2=f1;
             x1=c*a+(1-c)*b;
             f1=x1.^4+10*x1.*sin(x1.^2);
         else % move left boundary
             a=x1; x1=x2; f1=f2;
             x2=(1-c)*a + c*b;
             f2=x2.^4+10*x2.*sin(x2.^2);
         end

         if (b-a)<10^(-6) % break if close
             break
         end
     end
      #+END_SRC
      The above algorithm converges in _31 iterations_ to the minimum
      f = −10.0882 at x = −1.2742 with an accuracy of 10−6.

      A theorem regarding the golden search algorithm states that after k
      iterations, starting from the interval x ∈ [a, b], the midpoint of this
      final interval is within ck(b − a)/2 of the minimum. Thus a guaranteed
      convergence rate can be established.

**** Successive parabolic interpolation
***** 算法思想
      [Disadvantage of golden section]
      In the golden section search, no information was used about the values of
      f (x1) and f (x2) in selecting a new subinterval.

      [DONE]
      Thus if _f(x1)<< f(x2)_, it would be judicious to assume that the minimum
      might be _closer_ to the point _x1_ than x2 and _the interval should cut_
      _accordingly_.

      A technique that makes use of the function evaluation in choosing how to
      _refine the interval_ is the method of successive parabolic interpolation.
***** 算法图示
      #+NAME: Figure 5.2
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-04 19:19:25
      [[file:SciComput/screenshot_2017-05-04_19-19-25.png]]

      Figure 5.2: Graphical depiction of the successive parabolic interpolation
      algorithm. The three data points are depicted (circles) along with the
      point evaluated at the minimum of the parabola (star). The solid line is
      the function and the dotted line is the parabola generated. In this case,
      the guesses give a rapid convergence (14 iterations for 10^−6 accuracy) to
      the minimum.

***** 算法步骤
      #+NAME: successive parabolic interpolation
      #+BEGIN_QUOTE
      1. Choose 3 points:
         near the vicinity of the minimum: x1, x2 and x3. Evaluate each point, f(x1),f(x2),f(x3)
      2. Get new fn p(x):
         Using the _Lagrange polynomial coefficients_ to fit a parabola through 3 points.
      3. Get minimum x0 of p(x)
         by p'(x) = 0
      4. compare x0 with x2
         generate new 3 points x1', x2', x3'
      5. loop 2~5
      #+END_QUOTE

      #+NAME: how to compare x0 to x2
      #+BEGIN_SRC ditaa
     1.      x1          x2         x3
              .     ^    .
              .     |    .
              .     x0   .
              .     .    .
              .     .    .
     new:    x1     x2   x3

     2.      x1          x2         x3
                         .    ^      .
                         .    |      .
                         .    x0     .
                         .    .      .
                         .    .      .
     new:                x1   x2    x3
      #+END_SRC
***** 数学推论过程
      Using the Lagrange polynomial coefficients, this gives a parabolic function p(x)
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-04 19:24:36
      [[file:SciComput/screenshot_2017-05-04_19-24-36.png]]

      The minimum of the parabola now serves as a temporary proxy for the
      minimum of the actual function f(x). The minimum of the parabola can be
      found by setting the first derivative to zero so that we evaluate p'(x0)
      = 0. This gives, after some algebra, the following minimum:
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-04 21:02:19
      [[file:SciComput/screenshot_2017-05-04_21-02-19.png]]


      The idea now is to use this new point x0 as our new middle point x2. There are two cases of
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-04 21:02:45
      [[file:SciComput/screenshot_2017-05-04_21-02-45.png]]

      This gives a simple algorithm that progressively converges to the minimum
      by using information about the function values. Moreover, it only requires
      a single function evaluation per iterative step.
***** 算法优缺点
      *[Advantage: fast]*
      [[Figure 5.2]] gives a graphical depiction of this local iteration process.
      The convergence is typically _extremely fast_ once you can find a good
      neighborhood to work in.

      *[Disadvantage: not guarantee converge]*
      However, _the method is not guaranteed to converge_, unfortunately. Thus
      great care should be used with this method. Alternatively, very good
      starting points must always be used. The following code implements the
      successive parabolic approximation.

      However, _the method is not guaranteed to converge_ and can, in fact,
      easily diverge.

      Thus it is _important to have a good local starting point_.

***** Matlab 代码示例
      #+NAME: successive parabolic interpolation
      #+BEGIN_SRC matlab
      x1=-1.5; x2=-1; x3=-.5; % initial guesses
      f1=x1.^4+10*x1.*sin(x1.^2);
      f2=x2.^4+10*x2.*sin(x2.^2);
      f3=x3.^4+10*x3.*sin(x3.^2);
      for j=1:100
          x0 =(x1+x2)/2 - ( (f2-f1)*(x3-x1)*(x3-x2) )/( 2*( (x2-x1)*(f3-f2)-(f2-f1)*(x3-x2) ) );
          if x0>x2
              x1=x2; f1=f2;
              x2=x0; f2=x0.^4+10*x0.*sin(x0.^2);
          else
              x3=x2; f3=f2;
              x2=x0; f2=x0.^4+10*x0.*sin(x0.^2);
          end
          if abs(x2-x3)<10^(-6) | abs(x2-x1)<10^(-6)
              break
          end
      end
      #+END_SRC

      This algorithm converges to the solution in _less than half the iterations_
      _of the golden section search_.

      However, it is easy to show that if the initial guesses are changed, then
      the minimization will simply not work.
***** Matlab 内置函数: fminbnd
      _fminbnd MATLAB has a built-in one-dimensional search_ algorithm where
      the function and the interval are specified. The function fminbnd is
      based upon _a combination of the golden section search and successive_
      _parabolic search_. Integrated together they form an effective technique
      for finding minima. The following code gives an example of how to execute
      the function:

      #+NAME: fminbnd
      #+BEGIN_SRC matlab
      x=fminbnd('x^2*cos(x)',3,4)
      #+END_SRC

      #+RESULTS: fminbnd

      Here the left(x1) and right(x3) values of the search interval are given by
      x = 3 and x = 4,respectively.

      In this case, the function f(x) = x2 cos(x) was found to have a minimum at x = 3.6436.

*** 5.2 Unconstrained Optimization (Derivative Methods)
    The methods of the previous section do not utilize any derivative
    information about the objective function of interest. However, in many
    cases the explicit functional form to be considered for minimization is
    known, thus suggesting that derivatives may help in finding optimization
    solutions.

    Indeed, in simple one-dimensional problems for finding the
    minimum of f (x) = 0, it is well known that a minimum is found when f (x)
    = 0 and f (x) > 0. A maximum can be found when f (x) = 0 and f
    (x) < 0. Such ideas are easily integrated into an optimization
    algorithm.

    To begin, we generalize the concept of a minimum or maximum, i.e. an extremum for a multidimensional function f (x). At an extremum, the gradient must be zero so that
    ∇f (x) = 0 . (5.2.1)
    Unlike the one-dimensional case, there is no simple second derivative test to apply to determine if the extremum point is a minimum or maximum. The idea behind gradient descent, or
    steepest descent, is to use the derivative information as the basis of an iterative algorithm that
    progressively moves closer and closer to the minimum point f (x) = 0.
    To illustrate how to proceed in practice, consider the simple example two-dimensional surface
    f (x, y) = x2 + 3y2 (5.2.2)
    100 BASIC OPTIMIZATION
    which has the minimum located at the origin (x, y) = 0. The gradient for this function can be
    easily computed
    ∇f (x) = ∂f
    ∂x
    x ˆ + ∂f
    ∂y
    y ˆ = 2xx ˆ + 6yy ˆ (5.2.3)

    where x ˆ and y ˆ are unit vectors in the x- and y-directions, respectively.
    Figure 5.3 illustrates the steepest descent (gradient descent) algorithm. At the initial guess
    point, the gradient ∇f (x) can be computed. This gives the steepest descent towards the minimum point of f (x), i.e. the minimum is located in the direction given by −∇f (x). Note that the
    gradient does not point at the minimum, but rather gives the steepest path for minimizing f (x).
    The geometry of the steepest descent suggests the construction of an algorithm whereby the next
    point of iteration is picked by following the steepest descent so that
    ξ(τ) = x − τ∇f (x) (5.2.4)
    0
    3 2 1 0
    2 4
    5
    0
    0
    0
    50
    100
    –5 –5
    5
    5
    (a)
    (c)
    (b)
    (d)
    0
    0
    0
    0
    0
    50
    100
    –5
    –5 –5
    –5
    5
    5
    5
    Figure 5.3: Graphical depiction of the gradient descent algorithm. The surface f (x, y) = x2 + 3y2 is plotted
    along with its contour lines. In (a), the surface is plotted along with the contour plot beneath. In (b), the
    gradient is calculated at the point (x, y) = (3, 2). The gradient, which points away from the mininum, is
    plotted with the dark bolded line while the gradient line through (x, y) = (3, 2) is plotted with a dotted line.
    The gradient descent moves along the steepest line of descent as shown in panel (c). Once the bottom of the
    descent curve is reached, a new descent path is picked. Panel (d) shows the overall gradient of the surface in
    the upper right quadrant.
    UNCONSTRAINED OPTIMIZATION (DERIVATIVE METHODS) 101
    where the parameter τ dictates how far to move along the gradient descent curve. Figure 5.3(c)
    shows that the gradient descent curves gives a descent path that eventually reaches bottom and
    starts to go back up again. In gradient descent, it is crucial to determine when this bottom is
    reached so that the algorithm is always going downhill in an optimal way. This requires the
    determination of the correct value of τ in the algorithm.
    To compute the value of τ, consider the construction of a new function
    F(τ) = f (ξ(τ)) (5.2.5)
    which must be minimized now as a function of τ. This is accomplished by computing dF/dτ = 0.
    Thus one finds
    ∂F
    ∂τ
    = −∇f (ξ)∇f (x) = 0 . (5.2.6)
    The geometrical interpretation of this result is the following: ∇f (x) is the gradient direction of the
    current iteration point and ∇f (ξ) is the gradient direction of the future point, thus τ is chosen so
    that the two gradient directions are orthogonal.
    For the example given above with f (x, y) = x2 + 3y2, we can easily compute this conditions as
    follows:
    ξ = x − τ∇f (x) = (1 − 2τ)x x ˆ + (1 − 6τ)y y ˆ . (5.2.7)
    This is then used to compute
    F(τ) = f (ξ(τ)) = (1 − 2τ)2x2 + 3(1 − 6τ)2y2 (5.2.8)
    whereby its derivative with respect to τ gives
    F(τ) = −4(1 − 2τ)x2 − 36(1 − 6τ)y2 . (5.2.9)
    Setting F(τ) = 0 then gives
    τ =
    x2 + 9y2
    2x2 + 54y2 (5.2.10)
    as the optimal descent step length. This gives us all the information necessary to perform the
    steepest descent search for the minimum of the given function. As is clearly evident, this descent search algorithm based upon derivative information is very much like Newton’s method for
    root finding both in one dimension as well as higher dimensions. Moreover, the gradient descent
    algorithm is the core algorithm of advanced iterative solvers such as the bi-conjugate gradient
    descent method (bicgstab) and generalized method of residuals (gmres).
    In what follows, we develop a MATLAB code to perform the gradient descent search for the
    function f (x, y) = x2 + 3y2.
    102 BASIC OPTIMIZATION
    x(1)=3; y(1)=2; % initial guess
    f(1)=x(1)^2+3*y(1)^2; % initial function value
    for j=1:100
    tau=(x(j)^2 +9*y(j)^2)/(2*x(j)^2 + 54*y(j)^2);
    x(j+1)=(1-2*tau)*x(j); % update values
    y(j+1)=(1-6*tau)*y(j);
    f(j+1)=x(j+1)^2+3*y(j+1)^2;
    if abs(f(j+1)-f(j))<10^(-6) % check convergence
    break
    end
    end
    The above algorithm converges in only 11 iteration steps to the minimal solution (see Fig. 5.4).
    Interestingly enough, if a simple radially symmetric function is considered, then the gradient
    descent converges in a single iteration since the gradient descent would point directly at the minimum. As with other iterative schemes of this sort, including the root finding algorithms based
    upon the Newton method, convergence to the solution often depends on a user’s ability to provide
    a good initial guess for the minimal value.
    The above algorithm assumes a line search algorithm to find an optimal value of τ. In particular, the value of τ picked here is optimal in the sense that a given line search is conducted so
    −4 0 4
    −4
    4 0
    x
    y
    0 2 4 6 8
    10−6
    10−3
    100
    Iterations
    E
    Figure 5.4: Gradient descent algorithm applied to the function f (x, y) = x2 + 3y2. In the top panel, the
    contours are plotted for each successive value (x, y) in the iteration algorithm given the initial guess
    (x, y) = (3, 2). Note the orthogonality of each successive gradient in the steepest descent algorithm. The
    bottom panel demonstrates the rapid convergence and error (E) to the minimum (optimal) solution.
    UNCONSTRAINED OPTIMIZATION (DERIVATIVE METHODS) 103
    −4 0 4
    −4
    4 0
    x
    y
    0 2 4 6 8
    10−6
    10−3
    100
    Iterations
    E
    Figure 5.5: Gradient descent algorithm applied to the function f (x, y) = x2 + 3y2 with a fixed τ = 0.1.
    In the top panel, the contours are plotted for each successive value (x, y) in the iteration algorithm given
    the initial guess (x, y) = (3, 2). In this case, successive gradients are no longer orthogonal. The convergence and error (E) to the minimum (optimal) solution is slower with this line search method of a fixe
    value of τ.
    that the minimum of the gradient direction is picked as the next iteration point. However, this is
    not a requirement. In fact, one can simply choose a fixed value of τ for stepping forward along
    the gradient direction. Figure 5.5 demonstrates this case for τ = 0.1. This method also converges
    to the solution, however at a much slower rate. Such a method may be favorable in a case where
    the steepest descent algorithm zig-zags a large amount in trying to make the projective steps orthogonal. This can happen in cases where long-valley type structures exist in the function we are
    trying to minimize.
    fminsearch
    Although not based upon gradient descent algorithms, the fminsearch algorithm in MATLAB is
    a generic, nonlinear unconstrained optimization method based upon the Nelder–Mead simplex
    method [6]. We have already used this method as a means of doing nonlinear curve fitting.
    In that case, the objective function was the E2 error which was to be minimized. As a second
    example of this technique, consider once again a set of data that we wish to fit with the function
    f (x) = A cos(Bx) + C where A, B and C are the variables to be chosen for minimizing the error.
    Our objective function in this case is the least-square error E2 = (1/N)  |f (xj) − yj|2. Thus
    we only need to consider minimizing  |f (xj) − yj|2 with respect to A, B and C to achieve
    our goal.
    The following code performs the optimization process with initial guesses given by
    (A, B, C) = (12, π/12, 63).
    104 BASIC OPTIMIZATION
    c=fminsearch(’datafit’,[12 pi/12 63]); % optimization
    The function temp fit is given by
    datafit.m
    function e2=tempfit(c)
    x=1:24;
    y=[75 77 76 73 69 68 63 59 57 55 54 52 50 ...
    50 49 49 49 50 54 56 59 63 67 72];
    e2=sqrt(sum((c(1)*cos(c(2)*x)+c(3)-y).^2)/24);
    The algorithm will rapidly converge to new values of the vector c which contains the updated and
    optimal value of A, B and C. To plot the results and compare the fit (see Fig. 5.6), the following
    code is used:
    t=1:24; % raw data
    tem=[75 77 76 73 69 68 63 59 57 55 54 52 ...
    50 50 49 49 49 50 54 56 59 63 67 72];
    tt=1:0.01:24;
    yfit=(c(1)*cos(c(2)*tt)+c(3)).’;
    plot(t,tem,’ko’,tt,yfit,’k-’)
    0 6 12 18 24
    40
    50
    60
    70
    80
    x
    y
    Figure 5.6: Minimization algorithm fminsearch used for curve fitting to a nonlinear function. The dots are
    the original data points and the solid line is the least-square fit. In this case, the least-square error E2 is the
    objective function.
    LINEAR PROGRAMMING 105
    This example illustrates both the construction of an objective function as well as the implementation of one of the most important unconstrained optimization tools that is available in
    MATLAB. Critical to success in his algorithm is the initial guess used for the optimal (minimal)
    solution.
*** 5.3 Linear Programming
    We now come to perhaps the most import aspect in terms of application: optimization with
    constraint. This is still a highly active area of research and many methods exist which exploit the
    underlying nature of the problem being considered. Here, we will limit our discussion to a classic
    problem known as a linear program. A linear program is an optimization problem in which the
    objective function is linear in the unknown and the constraints consist of linear inequalities and
    equalities.
    To illustrate the linear programming concept, the so-called standard form will first be
    considered.
    minimize c1x1 + c2x2 + · · · + cnxn
    subject to a11x1 + a12x2 + · · · + a1nxn = b1
    a21x1 + a22x2 + · · · + a2nxn = b2
    ...
    (5.3.1)
    am1x1 + am2x2 + · · · + amnxn = bm
    and x1 ≥ 0, x2 ≥ 0, · · · , xn ≥ 0
    which can be written in a much more elegant form via vector and matrix notation
    minimize cTx (5.3.2)
    subject to Ax = b and x ≥ 0 .
    Thus given the matrix A and the vectors b and c, the goal is to find the vector x that minimizes
    the linear objective function given by c.
    Of course, not all linear optimization problems come directly in this form. But they can be
    transformed to the standard form by simple techniques.
    Slack variables
    Consider instead the following related problem which has inequality constraints instead of
    equality constraints.
    106 BASIC OPTIMIZATION
    minimize c1x1 + c2x2 + · · · + cnxn
    subject to a11x1 + a12x2 + · · · + a1nxn ≤ b1
    a21x1 + a22x2 + · · · + a2nxn ≤ b2
    ...
    (5.3.3)
    am1x1 + am2x2 + · · · + amnxn ≤ bm
    and x1 ≥ 0, x2 ≥ 0, · · · , xn ≥ 0.
    This problem is no longer in the standard form. However, it can be easily put into the standard
    form by introducing slack variables so that the inequalities can be made into equalities. Thus we
    transform the problem to the following:
    minimize c1x1 + c2x2 + · · · + cnxn
    subject to a11x1 + a12x2 + · · · + a1nxn + y1 = b1
    a21x1 + a22x2 + · · · + a2nxn + y2 = b2
    ...
    (5.3.4)
    am1x1 + am2x2 + · · · + amnxn + ym = bm
    and x1 ≥ 0, x2 ≥ 0, · · · , xn ≥ 0
    and y1 ≥ 0, y2 ≥ 0, · · · , yn ≥ 0.
    The introduction of the new m variables given by y now sets the problem to be in standard form.
    In particular, the new matrix A ¯ associated with the problem is now of the special form A ¯ = [A, I]
    where I is the identify matrix, and the new vector x ¯ to be solved for is x ¯ = [x, y].
    Other techniques exist to transform a linear optimization problem. If the inequalities are
    the opposite to the above, then surplus variables are introduced. If some of the unknown
    variables are actually not required to be positive, then they can be transformed using free variables [12]. MATLAB’s own built-in linear programming subroutine accepts a different form than
    the standard form, saving you the work of transforming it to this specific form.
    Any vector x that satisfies the constraints of (5.3.2) is a feasible solution. A feasible solution is
    called an optimal solution if, in addition, the objective function in (5.3.2), i.e. cTx, is minimal in
    comparison with all other feasible solutions. A basic feasible solution is one for which m − n of
    the variables x are zero, i.e. the number of nonzero solution elements is commensurate with the
    number of constraints. This leads to an important theorem of linear programming [12]:
    Fundamental theorem of linear programming: Given a linear program in the standard form (5.3.2)
    where A is an m × n matrix of rank m,
    (i) if there exists a feasible solution, there is a basic feasible solution.
    (ii) if there is an optimal feasible solution, there is an optimal basic feasible solution.
    The goal of linear programming is to find the optimal basic feasible solution of (5.3.2). As
    one might imagine, there have been a great number of mathematical techniques developed to
    LINEAR PROGRAMMING 107
    solve this critically important problem [12]. Here we will consider how to think about (5.3.2)
    graphically and then MATLAB’s linear programming function will be introduced.
    A graphical interpretation
    To illustrate the idea of feasible solutions, basic feasible solutions and transforming to the
    standard form, consider the following simple example
    minimize −2x1 − x2
    subject to x1 + (8/3)x2 ≤ 4
    x1 + x2 ≤ 2
    2x1 ≤ 3 (5.3.5)
    and x1 ≥ 0, x2 ≥ 0 .
    The idea is to first write this in the standard form by introducing three slack variables to handle
    the three constraint inequalities. Thus we have the new problem in standard form:
    minimize −2x1 − x2
    subject to x1 + (8/3)x2 + x3 = 4
    x1 + x2 + x4 = 2
    2x1 + x5 = 3 (5.3.6)
    and x1 ≥ 0, x2 ≥ 0, x3 ≥ 0, x4 ≥ 0, x5 ≥ 0
    where the slack variables are x3, x4 and x5. Given the three constraints, it is ideal to find a basic
    feasible solution that has two of the five variables set to zero.
    To begin discussing the solution of this problem, we first consider the region of feasibility
    solutions. Thus we consider x3 = 0 in the first constraint, x4 = 0 in the second constraint and
    x5 = 0 in the third constraint. Figure 5.7 demonstrates the feasibility region associated with
    0 0.5 1 1.5 2
    0
    0.5
    1
    1.5
    2
    x5= 0
    x3 = 0
    x4 = 0
    x
    1
    x
    2
    f= −3
    f = −1 f= −2
    Figure 5.7: Graphical representation of the feasible region (shaded) given the constraints (5.3.5). The constraints are represented in terms of the slack variables. The objective function f = cTx is evaluated along
    contour lines. Thus the linear program seeks to minimize f while satisfying the constraints, i.e. the linear
    program would identify the point (x1, x2) = (1.5, 0.5) as the optimal basic feasible solution.
    108 BASIC OPTIMIZATION
    this example. Once the feasibility solution is found, our objective is to minimize the objective
    function
    min f (x1, x2) = min cTx = −2x1 − x2 . (5.3.7)
    Figure 5.7 also demonstrates the lines of constant f . Note that the value of f decreases as the
    line of constant f is pushed to the right. The point (x1, x2) = (1.5, 0.5) is the furthest point in the
    feasible region that one can push to the right, thus it is the optimal solution. Moreover, it is a
    basic optimal solution since x4 = x5 = 0 at this solution point.
    linprog
    Of course, what is desired is a systematic way to find the basic optimal solution. In the example
    given previously, it was simple to see from plotting alone where the optimal solution would be.
    However, in higher dimensional problems, the aid of such graphical techniques is rarely available.
    Thus algorithmic constructs for finding feasible solutions, and then iterating towards the optimal
    feasible solution, are of primary importance. Such methods have been developed; for example,
    the simplex method and/or interior point methods. These fall outside the scope of this book, but
    they can be followed up on in the literature [12].
    Here, MATLAB’s linear program subroutine, linprog, will be considered. This is an extremely
    powerful tool for solving linear programming problems. The form of the linear program used
    by MATLAB is slightly different from the standard form. In particular, MATLAB will solve the
    following problem:
    minimize cTx (5.3.8)
    subject to Ax ≤ b
    Ax ¯ = b ¯
    x
    − ≤ x ≤ x+ (5.3.9)
    where x
    − and x+ are lower and upper bounds on the values of x, respectively. Note that in
    this formulation, the equality and inequality constraints are separated. MATLAB automatically
    formulates the slack/surplus variables for you.
    The example given previously can be rewritten as
    minimize −2x1 − x2
    subject to x1 + (8/3)x2 ≤ 4
    x1 + x2 ≤ 2
    2x1 ≤ 3 (5.3.10)
    −x1 ≤ 0 (5.3.11)
    −x2 ≤ 0 .
    LINEAR PROGRAMMING 109
    In the matrix form as required by (5.3.8), one would then have
    A =
    ⎡⎢⎢⎢⎢⎢⎣
    1 8/3
    1 1
    2 0
    −1 0
    0 −1
    ⎤⎥⎥⎥⎥⎥⎦
    , b =
    ⎡⎢⎢⎢⎢⎢⎣
    42300
    ⎤⎥⎥⎥⎥⎥⎦
    , c = − −2 1  . (5.3.12)
    Note that in this case, there are no equality constraints so that A ¯ and b ¯ do not need to be defined.
    Nor do we need to define bounds on the solution. The MATLAB code for implementing this
    linear program is as follows:
    c=[-2 -1];
    A=[1 8/3; 1 1; 2 0; -1 0; 0 -1];
    b=[4; 2; 3; 0; 0];
    x = linprog(c,A,b)
    This produces the optimal solution (x1, x2) = (1.5, 0.5).
    More generally, the linprog is of the following form
    [x,fval,exitflag]= linprog(c,A,b,Abar,bbar,xl,xu,x0,options)
    where xl and xu correspond to the lower and upper bound vectors, Abar and bbar are the
    matrices corresponding to the equality constraints, x0 is an initial guess for the solution if available, and the options allow for toggling of the error tolerance, for instance. Upon return, the
    variable exitflag describes if the optimization routine converged (it is equal to unity), or wether
    the maximum number of iterations were performed without converging, or if something else
    went wrong in the linear programming procedure.
    Open source optimization packages: cvx
    Of course, linear programming can be quite restrictive since it is, in fact, limited to linear objective functions and linear constraints. There are methods available for nonlinear programming [12],
    however, they are beyond the scope of this book. Thankfully, there are a number of open source
    convex optimization codes that can be downloaded from the Internet. In the compressive sensing chapter to come, a convex optimization package is used that can be directly implemented
    with MATLAB: http://cvxr.com/cvx/. This is one of several codes that can be downloaded that
    use state-of-the-art optimization techniques that go far beyond both the constraints of linear
    programming.
    110 BASIC OPTIMIZATION
*** 5.4 Simplex Method
    Before moving on from the linear programming method, a key issue must be addressed: From a
    feasible solution, how can new feasible solutions be generated that are more optimal? Indeed, how
    can one find the optimal solution, which is the solution of the linear programming algorithm.
    Here, the simplex method is discussed which was developed by G.B. Dantzig in 1948. As is expected, the simplex method is a systematic iterative technique which aims to take a given basic
    feasible solution to another basic feasible solution for which the objective function is smaller.
    Consider once again the linear program in standard form:
    minimize f (x) = c1x1 + c2x2 + · · · + cnxn
    subject to a11x1 + a12x2 + · · · + a1nxn = b1
    a21x1 + a22x2 + · · · + a2nxn = b2
    ...
    (5.4.1)
    am1x1 + am2x2 + · · · + amnxn = bm
    and x1 ≥ 0, x2 ≥ 0, · · · , xn ≥ 0
    where we have now represented the objective function as f (x).
    First, we can easily consider the constraint conditions and feasibility. Specifically, the constraint equations are simply Ax = b where A is an m × n matrix where m < n. Thus the
    constraint system is underdetermined and there are an infinite number of possible solutions (see
    Fig. 5.7 which shows the entire (shaded) region of infinite solutions). Thus since we are guaranteed a solution to the underdetermined system, we are guaranteed a feasible solution. But once
    this feasible solution is found, we can easily put it into the form of a basic feasible solution by
    converting it (via Gaussian elimination type techniques) to the canonical form:
    x1 + y1,m+1xm+1 + y1,m+2xm+2 + · · · + y1,nxn = y10
    x2 + y2,m+1xm+1 + y2,m+2xm+2 + · · · + y2,nxn = y20
    ...
    (5.4.2)
    xm + ym,m+1xm+1 + ym,m+2xm+2 + · · · + ym,nxn = ym0 .
    Once in the canonical form, a basic feasible solution is found where
    x1 = y10, x2 = y20, xm = ym0, and xm+1 = 0, xm+2 = 0, · · · , xn = 0 . (5.4.3)
    This canonical solution is also a basic feasible solution. The variables x1, x2, · · · , xm are called
    basic and the variables xm+1, xm+2, · · · , xn are called nonbasic.
    Here is the fundamental question to ask: Do we have the right basic and nonbasic variables?
    Specifically, what if there is a variable xp, where p is from somewhere in m + 1 to n, such that
    it would be a better choice as a basic variable, i.e. it would give a more optimal solution where
    the objective function is smaller. The simplex method fundamentally is concerned with making
    SIMPLEX METHOD 111
    basic those variables that, in fact, give an optimal solution. Thus an iteration procedure must be
    created to perform such an action.
    To move forward, the simplex tableau is created for the above basic feasible solution. Thus we
    can write this in a more shorthand notation as:
    1 0 · · · 0 y1,m+1 y1,m+2 · · · y1,n y1,0
    0 1 · · · 0 y2,m+1 y2,m+2 · · · y2,n y2,0
    · · · · · · · · ·
    · · · · · · · · ·
    0 0 · · · 1 ym,m+1 ym,m+2 · · · ym,n ym,0.
    (5.4.4)
    The purpose in writing it in this form is that the operations which will be performed for the simplex method are much like those in Gaussian elimination. In particular, it is often advantageous
    to switch basic and nonbasic variables, thus necessitating column and row reductions to achieve
    this goal.
    Here is the critical observation, and the fundamental point, of the simplex method. Although it
    is natural to use the basic solutions from the computed tableau above, it is also clear that arbitrary
    values of xm+1, xm+2, · · · , xn can be chosen. Recall that this is an underdetermined system, so an
    infinite number of solutions are allowed, including those with nontrivial nonbasic variables. If
    these nontrivial basic variables are chosen arbitrarily, then the above tableau gives the following
    values of the basic variables:
    x1 = y10 − n j=m+1 y1,jxj
    x2 = y20 − n j=m+1 y2,jxj
    · · ·
    · · ·
    xm = ym0 − n j=m+1 ym,jxj .
    (5.4.5)
    Of course, this is a trivial observation. But it has a profound impact when considering the
    objective function
    f = c1x1 + c2x2 + · · · + cnxn (5.4.6)
    = f0 + (cm+1 − fm+1)xm+1 + (cm+2 − fm+2)xm+2 + · · · + (cn − fn)xn
    where
    fj = y1,jc1 + y2,jc2 + · · · + ym,jcm (5.4.7)
    with m + 1 ≤ j ≤ n. The critical observation is that this formulation gives the value of the objective function f (x) in terms of the nonbasic variables xm+1, xm+2, · · · , xn. Thus from this we can
    determine if there is an advantage to switching basic to nonbasic variables in order to minimize
    the objective function. Specifically, if any (cj − fj) < 0 in the above formula, then the objective
    function will be lowered. The following theorem then applies:
    Theorem (Improvement of basic feasible solution): Given a nondegenerate basic feasible solution
    with corresponding objective function f0, suppose that for some j there holds (cj − zj) < 0. Then
    there is a feasible solution with objective function value f < f0. If the column aj can be substituted
    112 BASIC OPTIMIZATION
    for some vector in the original basis to yield a new basic feasible solution, this new solution will
    have f < f0. If aj cannot be substituted to yield a basic feasible solution, then the solution set is
    unbounded and the objective function can be made arbitrarily small (toward minus infinity).
    The above theorem is the basis for the simplex method. Thus from a given basic feasible solution, it only remains to identify (cj − zj) < 0 and use pivoting and row reduction techniques to
    swap basic and nonbasic solutions so that a new, and smaller, objective function is achieved. This
    process is continued until no (cj − zj) < 0 remain. In fact, the following optimality theorem then
    holds:
    Theorem (Optimality condition theorem): If for some basic feasible solution (cj − zj) ≥ 0 for all
    j, then the solution is optimal.
    Armed with the above two theorems, the simplex method can be constructed and a termination
    point reached in the iteration method. Note that just like Gaussian elimination, which involves
    the same basic procedures in row and column reductions and manipulations, the larger the
    matrix, the greater the time in computation.
    To illustrate how the actual process is achieved, consider the objective function f (x) =  xj
    and the following simplex tableau:
    1 0 0 2 4 6 4
    0 1 0 1 2 3 3
    0 0 1 −1 2 1 1
    (5.4.8)
    where the first six columns correspond to the coefficients of x1, x2, x3, x4, x5 and x6. The final
    column is the coefficients of the constraint vector b. The basic feasible solution in this case is
    x = (4, 3, 1, 0, 0, 0) with f = xj = 8. (5.4.9)
    Now suppose we elect to bring the fourth column a4 into the basis, i.e. make it a basic versus
    nonbasic variable. Then it is necessary to determine which element in the fourth column is the
    appropriate pivot. The following three ratios are computed
    b(1)/y1,4 = 4/2 = 2, b(2)/y2,4 = 3/1 = 3, b(3)/y3,4 = 1/ − 1 = −1. (5.4.10)
    The idea is to choose the smallest positive pivot, thus the pivot point will be the b(1)/y1,4 = 2
    term and the pivoting happens about the first row, fourth column. As with Gaussian elimination,
    the goal is to make the other elements of the fourth column zero which can be achieved by adding
    and subtracting appropriately scaled rows. This yields the new simplex tableau:
    1/2 0 0 1 2 3 2
    −1/2 1 0 0 0 0 1
    1/2 0 1 0 4 4 3
    (5.4.11)
    which has the basic feasible solution
    x = (0, 1, 3, 2, 0, 0) with f = xj = 6 . (5.4.12)
    GENETIC ALGORITHMS 113
    This simple example shows that the objective function was reduced from 8 to 6 simply by
    switching a basic with nonbasic variable.
    More generally, the simplex algorithm proceeds as follows:
    (i) Form a simplex tableau from the initial basic feasible solution and compute the (cj − fj).
    (ii) If each (cj − fj) ≥ 0, stop the algorithm since the basic feasible solution is optimal.
    (iii) Select the jth column for which (cj − fj) < 0 is the least negative. This column will be made
    into a basic variable.
    (iv) Determine all the potential pivot values by evaluating yk0/ykj for ykj > 0 and
    k = 1, 2, · · · , m. If no ykj > 0, then stop as the problem is unbounded. Otherwise, select
    p as the index k corresponding to the minimum ratio.
    (v) Pivot on the pjth element, updating all rows including the last. Return to the first step (i).
    This gives the basic outline of the technique. Of course, just like Gaussian elimination, certain problems can arise in the pivoting process, including if there is degeneracy in the system.
    There are numerous techniques and algorithm improvements for the simplex method, and one
    is encouraged to follow these up in the literature [12].
*** 5.5 Genetic Algorithms
    Other methods developed for optimization problems are the so-called genetic algorithms which
    are a subset of evolutionary algorithms. The principle is quite simple and mirrors what is perceived to occur in evolution and/or genetic mutations. In particular, given a set of feasible trial
    solutions (either constrained or unconstrained), the objective function is evaluated. In the language of genetic algorithms, the objective function is now called the fitness function. The idea is
    to keep those solutions that give the minimal value of the objective function and mutate them in
    order to try and do even better. Thus beneficial mutations, in the sense of giving a better minimization, are kept while those that perform poorly are thrown away, i.e. survival of the fittest. This
    process is repeated through a prescribed number of iterations, or generations, with the idea that
    better and better fitness function values are generated via the mutation process.
    To be more precise about the genetic algorithm structure, consider the unconstrained optimization problem with the objective function
    min f (x) (5.5.1)
    where x is an n-dimensional vector. Suppose that m initial guesses are given for the values of x so
    that
    guess j is xj . (5.5.2)
    Thus m solutions are evaluated and compared with each other in order to see which of the solutions generate the smallest objective function since our goal is to minimize it. We can order
    114 BASIC OPTIMIZATION
    the guesses so that the first p < m give the smallest values of f (x). Arranging our data, we
    then have
    keep xj j = 1, 2, · · · , p (5.5.3)
    discard xj j = p + 1, p + 2, · · · , m .
    Since the first p solutions are the best, these are kept in the next generation. In addition, we now
    generate m − p new trial solutions that are randomly mutated from the p best solutions. This
    process is repeated through a finite number of iterations with the hope that convergence to the
    optimal solution is achieved.
    Interestingly, there are really no theorems about the convergence of such a technique, so one
    may wonder why it should be considered at all when guaranteed convergence can be achieved
    with alternative algorithms. The use of such algorithms is due to a few key advantages that are
    difficult to find elsewhere. First, many iteration schemes can get stuck in the iteration process for a
    variety of reasons. Thus convergence is extremely slow, or only a local minimum can be found. In
    the mutation process of the genetic algorithm, there exists the possibility of moving well beyond
    these pernicious points so that the iteration can continue moving towards the optimal solution.
    Second, in all that has been considered thus far, an optimization problem can be neatly packaged
    as a set of constraints with an objective function. However, suppose the problem is sufficiently
    complex so that nonlinear constraints exist and the methods developed previously simply no
    longer hold. Alternatively, what if the objective function to be minimized is only computable
    after a larger simulation has been performed? Thus the idea is to choose the parameters of this
    larger simulation based upon the genetic algorithm itself and its ability to minimize the objective
    function.
    To demonstrate the concept, consider the example in Fig. 5.6 which was solved using the fminsearch algorithm. In this case, the function form f (x) = A cos(Bx) + C is assumed and the fitness
    function (objective function) is the E2 error. Here, a genetic algorithm will be developed that will
    search for the optimal solution using a set of initial guesses followed by mutations of the best
    solutions. We begin by defining some initial parameters for the genetic algorithm. In particular,
    200 generations will be run with 50 trial solutions. Only the top 10 best solutions will be kept and
    mutated at the next generation. As before with fminsearch, an initial guess of A = 12, B = pi/12
    and C = 60 will be used.
    m=200; % number of generations
    n=50; % number of trials
    n2=10; % number of trials to be kept
    A=12+randn(n,1); B=pi/12+randn(n,1); C=60+randn(n,1);
    The main loop of the genetic algorithm is now ready to be performed. Below, the objective function is evaluated for each trial solution. The trials are then ordered from smallest to largest and
    the best 10 are kept. Of these 10, four mutations are made of each and the process repeated.
    GENETIC ALGORITHMS 115
    for jgen=1:m
    for j=1:n % evaluate objective function
    E(j)= sum((A(j)*cos(B(j)*x)+C(j)-y).^2);
    end
    [Es,Ej]=sort(E); % sort from small to large
    Ak1=A(Ej(1:n2)); % best 10 solutions
    Bk1=B(Ej(1:n2));
    Ck1=C(Ej(1:n2));
    Ak2=Ak1+randn(n2,1)/jgen; % 10 new mutations
    Bk2=Bk1+randn(n2,1)/jgen;
    Ck2=Ck1+randn(n2,1)/jgen;
    Ak3=Ak1+randn(n2,1)/jgen; % 10 new mutations
    Bk3=Bk1+randn(n2,1)/jgen;
    Ck3=Ck1+randn(n2,1)/jgen;
    Ak4=Ak1+randn(n2,1)/jgen; % 10 new mutations
    Bk4=Bk1+randn(n2,1)/jgen;
    Ck4=Ck1+randn(n2,1)/jgen;
    Ak5=Ak1+randn(n2,1)/jgen; % 10 new mutations
    Bk5=Bk1+randn(n2,1)/jgen;
    Ck5=Ck1+randn(n2,1)/jgen;
    A=[Ak1; Ak2; Ak3; Ak4; Ak5]; % group new 50
    B=[Bk1; Bk2; Bk3; Bk4; Bk5];
    C=[Ck1; Ck2; Ck3; Ck4; Ck5];
    end
    Note that the algorithm takes progressively smaller mutations as the generations progress, thus
    the divide by jgen. Although a contrived example, this genetic algorithm converges nicely to
    the least-square solution. It should be noted, however, that this is an extremely slow method for
    doing curve fitting. So this example should be thought of as illustrative only. The convergence of
    the scheme and the data fit can be found in Fig. 5.8. Figure 5.9 shows the error of the 50 trials at
    various generations of the algorithm.
    116 BASIC OPTIMIZATION
    0 50 100 150 200
    100
    150
    200
    generations
    E
    0 6 12 18 24
    40
    50
    60
    70
    80
    x
    y
    Figure 5.8: The top panel shows the data and curve fit from the genetic algorithm as developed here (solid
    line) and MATLAB’s genetic algorithm (dotted line). The bottom panel shows the error of the best solution
    at each successive generation. The error slowly converges to the same solution as fminsearch.
    0 10 20 30 40 50
    0
    4000
    8000
    Generation = 1
    0 10 20 30 40 50
    0
    2000
    4000
    Generation =10
    0 10 20 30 40 50
    0
    1000
    2000
    trial
    E
    Generation = 50
    0 10 20 30 40 50
    0
    500
    1000
    Generation =100
    Figure 5.9: Error of the 50 trial solutions at generation 1, 10, 50 and 100. Note the convergence to the
    optimal solution as generations progress forward.
    GENETIC ALGORITHMS 117
    ga
    MATLAB also has a built-in genetic algorithm code that is easy to use and implement. Moreover,
    it can be set up to do constrained optimization problems, even with nonlinear objective functions.
    To begin, the ga algorithm is illustrated with the simple example of the curve fit just illustrated.
    The code for solving this problem is given by
    lower=[10 pi/20 50];
    upper=[15 pi/4 70];
    x=ga(@(x)fit_line(x),3,[],[],[],[],lower,upper)
    where the objective function is given by
    fit_line.m
    function E=fit_line(x)
    xx=1:24;
    yy=[75 77 76 73 69 68 63 59 57 55 54 52 ...
    50 50 49 49 49 50 54 56 59 63 67 72];
    E=sum((x(1)*cos(x(2)*xx)+x(3)-yy).^2);
    Note that in the code, a lower and upper bound on the solution has been provided. This is equivalent to providing a good initial guess. If upper and lower bounds are not provided, then the
    algorithm fails completely.
    More generally, the genetic algorithm as developed by MATLAB allows for both equality and
    inequality constraints. Additionally, nonlinear constraints can be imposed on the problem, thus
    making the ga algorithm extremely powerful. To be specific, the following problem can be solved
    minimize f (x) (5.5.4)
    subject to Ax ≤ b
    Ax ¯ = b ¯
    g(x) ≤ 0
    g ¯(x) = 0
    x
    − ≤ x ≤ x+ . (5.5.5)
    The generic optimization thus allows for a nonlinear objective function f (x) along with a set of
    linear equality and inequality constraints, Ax ≤ b and Ax ¯ = b ¯, respectively, a set of nonlinear
    118 BASIC OPTIMIZATION
    equality and inequality constraints, g(x) ≤ 0 and g ¯(x) = 0, respectively, and upper and lower
    bounds, x+ and x−, respectively. A function call to the ga algorithm is given by
    x = ga(’fit’,n,A,b,Abar,bbar,xl,xu,nonlin,options)
    where the above constraints are placed one by one into the genetic algorithm. The options may
    become important as the maximum number of generations and tolerance, for instance, are set
    within this variable space.
